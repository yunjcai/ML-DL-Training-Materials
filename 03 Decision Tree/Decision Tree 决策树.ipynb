{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 什么是决策树\n",
    "\n",
    "### 1.1 决策树的基本思想\n",
    "\n",
    "其实用一下图片能更好的理解LR模型和决策树模型算法的根本区别，我们可以思考一下一个决策问题：是否去相亲，一个女孩的母亲要给这个女孩介绍对象。\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/03%20Decision%20Tree/image/1.jpg?raw=true\" width=\"500\">\n",
    "\n",
    "大家都看得很明白了吧！LR 模型是一股脑儿的把所有特征塞入学习，而决策树更像是编程语言中的 if-else 一样，去做条件判断，这就是根本性的区别。\n",
    "\n",
    "### 1.2 “树”的成长过程\n",
    "\n",
    "决策树基于“树”结构进行决策的，这时我们就要面临两个问题 ：\n",
    "\n",
    "- “树”怎么长。\n",
    "- 这颗“树”长到什么时候停。\n",
    "\n",
    "弄懂了这两个问题，那么这个模型就已经建立起来了，决策树的总体流程是“分而治之”的思想，一是自根至叶的递归过程，一是在每个中间节点寻找一个“划分”属性，相当于就是一个特征属性了。接下来我们来逐个解决以上两个问题。\n",
    "\n",
    "#### 这颗“树”长到什么时候停\n",
    "\n",
    "- 当前结点包含的样本全属于同一类别，无需划分；例如：样本当中都是决定去相亲的，属于同一类别，就是不管特征如何改变都不会影响结果，这种就不需要划分了。\n",
    "- 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分；例如：所有的样本特征都是一样的，就造成无法划分了，训练集太单一。\n",
    "- 当前结点包含的样本集合为空，不能划分。\n",
    "\n",
    "### 1.3 \"树\"怎么长\n",
    "\n",
    "在生活当中，我们都会碰到很多需要做出决策的地方，例如：吃饭地点、数码产品购买、旅游地区等，你会发现在这些选择当中都是依赖于大部分人做出的选择，也就是跟随大众的选择。其实在决策树当中也是一样的，当大部分的样本都是同一类的时候，那么就已经做出了决策。\n",
    "\n",
    "#### 1.3.1 ID3算法\n",
    "\n",
    "我们可以把大众的选择抽象化，这就引入了一个概念就是纯度，想想也是如此，大众选择就意味着纯度越高。好，在深入一点，就涉及到一句话：**信息熵 (entropy) 越低，纯度越高**。我相信大家或多或少都听说过“熵”这个概念，信息熵通俗来说就是用来度量包含的“信息量”，如果样本的属性都是一样的，就会让人觉得这包含的信息很单一，没有差异化，相反样本的属性都不一样，那么包含的信息量就很多了。\n",
    "\n",
    "一到这里就头疼了，因为马上要引入信息熵 (entropy) 的公式，其实也很简单：\n",
    "\n",
    "$Ent(D)=-\\sum_{k=1}^{|y|}p_klog_2p_k$\n",
    "\n",
    "$P_k$ 表示的是：当前样本集合 D 中第 k 类样本所占的比例为 $P_k$。\n",
    "\n",
    "**信息增益**\n",
    "\n",
    "废话不多说直接上公式：\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/03%20Decision%20Tree/image/2.jpg?raw=true\" width=\"500\">\n",
    "\n",
    "看不懂的先不管，简单一句话就是：划分前的信息熵--划分后的信息熵。表示的是向纯度方向迈出的“步长”。\n",
    "\n",
    "解释：在根节点处计算信息熵，然后根据属性依次划分并计算其节点的信息熵，用根节点信息熵--属性节点的信息熵=信息增益，根据信息增益进行降序排列，排在前面的就是第一个划分属性，其后依次类推，这就得到了决策树的形状，也就是怎么“长”了。\n",
    "\n",
    "如果不理解的，可以查看我分享的图片示例，结合我说的，包你看懂：\n",
    "\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/03%20Decision%20Tree/image/3.jpeg?raw=true\" width=\"500\">\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/03%20Decision%20Tree/image/4.jpeg?raw=true\" width=\"500\">\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/03%20Decision%20Tree/image/5.jpeg?raw=true\" width=\"500\">\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/03%20Decision%20Tree/image/6.jpeg?raw=true\" width=\"500\">\n",
    "\n",
    "不过，信息增益有一个问题：对可取值数目较多的属性有所偏好，例如：考虑将“编号”作为一个属性。为了解决这个问题，引出了另一个算法 C4.5。\n",
    "\n",
    "#### 1.3.2 C4.5\n",
    "\n",
    "为了解决信息增益的问题，引入一个信息增益率：\n",
    "\n",
    "$Gain\\_ratio(D,a)=\\frac{Gain(D,a)}{IV(a)}$\n",
    "\n",
    "其中：\n",
    "\n",
    "$IV(a)=-\\sum_{v=1}^{V}\\frac{|D^v|}{|D|}log_2\\frac{|D^v|}{|D|}$\n",
    "\n",
    "属性 a 的可能取值数目越多(即 V 越大)，则 IV(a) 的值通常就越大。**信息增益比本质： 是在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。**不过有一个缺点：\n",
    "\n",
    "- 缺点：信息增益率偏向取值较少的特征。\n",
    "\n",
    "使用信息增益率：基于以上缺点，并不是直接选择信息增益率最大的特征，而是现在候选特征中找出信息增益高于平均水平的特征，然后在这些特征中再选择信息增益率最高的特征。\n",
    "\n",
    "#### 1.3.3 CART算法\n",
    "\n",
    "Classification And Regression Tree (CART) 是决策树的一种，CART 算法既可以用于创建分类树（Classification Tree），也可以用于创建回归树（Regression Tree）\n",
    "\n",
    "数学家真实聪明，想到了另外一个表示纯度的方法，叫做基尼指数(Gini index)：\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/03%20Decision%20Tree/image/7.jpg?raw=true\" width=\"350\">\n",
    "\n",
    "其中 $p_k$ 表示第 k 类的概率。**Gini(D) 越小，数据集 D 的纯度越高。**\n",
    "\n",
    "##### 举个例子\n",
    "\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/03%20Decision%20Tree/image/11.png?raw=true\" width=\"300\">\n",
    "\n",
    "在上述图中，属性有 3 个，分别是有房情况，婚姻状况和年收入，其中有房情况和婚姻状况是离散的取值，而年收入是连续的取值。拖欠贷款者属于分类的结果。\n",
    "\n",
    "假设现在来看有房情况这个属性，那么按照它划分后的 Gini 指数计算如下\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/03%20Decision%20Tree/image/12.png?raw=true\" width=\"350\">\n",
    "\n",
    "而对于婚姻状况属性来说，它的取值有 3 种，按照每种属性值分裂后 Gini 指标计算如下\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/03%20Decision%20Tree/image/13.jpg?raw=true\" width=\"400\">\n",
    "\n",
    "最后还有一个取值连续的属性，年收入，它的取值是连续的，那么连续的取值采用分裂点进行分裂。如下\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/03%20Decision%20Tree/image/14.png?raw=true\" width=\"350\">\n",
    "\n",
    "最好的划分就是使得 GINI 最小的划分 -- 单身或离异 / 已婚\n",
    "\n",
    "根据这样的分裂规则 CART 算法就能完成建树过程。 建树完成后就进行第二步了，即根据验证数据进行剪枝。在 CART 树的建树过程中，可能存在 Overfitting，许多分支中反映的是数据中的异常，这样的决策树对分类的准确性不高，那么需要检测并减去这些不可靠的分支。决策树常用的剪枝有事前剪枝和事后剪枝，CART 算法采用事后剪枝，具体方法为代价复杂性剪枝法。\n",
    "\n",
    "##### 终止条件\n",
    "一个节点产生左右孩子后，递归地对左右孩子进行划分即可产生分类回归树。这里的终止条件是什么？什么时候节点就可以停止分裂了？直观的情况，当节点包含的数据记录都属于同一个类别时 (当$Gini(D) = 0$) 就可以终止分裂了。\n",
    "\n",
    "\n",
    "#### 1.3.4 三种不同的决策树\n",
    "\n",
    "- **ID3**：取值多的属性，更容易使数据更纯，其信息增益更大。训练得到的是一棵庞大且深度浅的树：不合理。\n",
    "\n",
    "- **C4.5**：采用信息增益率替代信息增益。\n",
    "\n",
    "- **CART**：以基尼系数 (Gini index) 替代熵 (entropy)，最小化不纯度，而不是最大化信息增益。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 决策树如何剪枝\n",
    "\n",
    "决策树的剪枝基本策略有 预剪枝 (Pre-Pruning) 和 后剪枝 (Post-Pruning)。\n",
    "\n",
    "- **预剪枝**：其中的核心思想就是，在每一次实际对结点进行进一步划分之前，先采用验证集的数据来验证如果划分是否能提高划分的准确性。如果不能，就把结点标记为叶结点并退出进一步划分；如果可以就继续递归生成节点。\n",
    "- **后剪枝**：后剪枝则是先从训练集生成一颗完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来泛化性能提升，则将该子树替换为叶结点。\n",
    "\n",
    "参考文章：[决策树及决策树生成与剪枝](https://blog.csdn.net/am290333566/article/details/81187562)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. [代码实现](https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/03%20Decision%20Tree/demo/DecisionTree.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
