{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.什么是线性回归\n",
    "\n",
    "- 线性：两个变量之间的关系**是**一次函数关系的——图象**是直线**，叫做线性。\n",
    "- 非线性：两个变量之间的关系**不是**一次函数关系的——图象**不是直线**，叫做非线性。\n",
    "- 回归：人们在测量事物的时候因为客观条件所限，求得的都是测量值，而不是事物真实的值，为了能够得到真实值，无限次的进行测量，最后通过这些测量数据计算**回归到真实值**，这就是回归的由来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 能够解决什么样的问题\n",
    "\n",
    "对大量的观测数据进行处理，从而得到比较符合事物内部规律的数学表达式。也就是说寻找到数据与数据之间的规律所在，从而就可以模拟出结果，也就是对结果进行预测。解决的就是通过已知的数据得到未知的结果。例如：对房价的预测、判断信用评价、电影票房预估等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 一般表达式是什么\n",
    "\n",
    "$Y = wx+b$\n",
    "\n",
    "$w$ 叫做 $x$ 的系数，$b$ 叫做偏置项。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 如何计算\n",
    "\n",
    "### Loss Function--MSE\n",
    "\n",
    "$J_0=\\frac{1}{2m}\\sum^{i=1}_{m}(y^{'}-y)^2$\n",
    "\n",
    "利用 [**梯度下降法**](https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/Gradient%20Descent/Gradient%20Descent%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.ipynb) 找到最小值点，也就是最小误差，最后把 $w$ 和 $b$ 给求出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 过拟合、欠拟合如何解决\n",
    "\n",
    "使用正则化项，也就是给 loss function 加上一个参数项，正则化项有**$L1$ 正则化、$L2$ 正则化、ElasticNet**。加入这个正则化项好处：\n",
    "\n",
    "- 控制参数幅度，不让模型“无法无天”。\n",
    "- 限制参数搜索空间\n",
    "- 解决欠拟合与过拟合的问题。\n",
    " \n",
    "### 5.0 什么是 $L1$ 正则化、$L2$ 正则化\n",
    "* <font color='red'>**$L1$ as reducing the number of features in the model altogether.**</font> \n",
    "* <font color='red'>**$L2$ “regulates” the feature weight instead of just dropping them.**</font>\n",
    "\n",
    "### 5.1 什么是 L2 正则化(岭回归)\n",
    "\n",
    "方程：\n",
    "\n",
    "$J=J_0+\\lambda\\sum_{w}w^2$\n",
    "\n",
    "$J_0$表示上面的 loss function ，在 loss function 的基础上加入 $w$ 参数的平方和乘以 $\\lambda$\n",
    "\n",
    "假设：\n",
    "\n",
    "$L=\\lambda({w_1}^2+{w_2}^2)$\n",
    "\n",
    "回忆以前学过的单位元的方程：\n",
    "\n",
    "$x^2+y^2=1$\n",
    "\n",
    "正和 $L2$ 正则化项一样，此时我们的任务变成在 $L$ 约束下求出 $J$ 取最小值的解。求解 $J_0$ 的过程可以画出等值线。同时 $L2$ 正则化的函数 $L$ 也可以在 $w1$ $w2$ 的二维平面上画出来。如下图：\n",
    "\n",
    "![image](https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/01%20Linear%20Regression/image/1.jpg?raw=true)\n",
    "\n",
    "$L$ 表示为图中的黑色圆形，随着梯度下降法的不断逼近，与圆第一次产生交点，而这个交点很难出现在坐标轴上。这就说明了 $L2$ 正则化不容易得到稀疏矩阵，同时为了求出损失函数的最小值，使得 $w1$ 和 $w2$ 无限接近于 $0$，达到防止过拟合的问题。\n",
    "\n",
    "### 5.2 什么场景下用 L2 正则化\n",
    "\n",
    "只要数据线性相关，用 LinearRegression 拟合的不是很好，**需要正则化**，可以考虑使用岭回归 ($L2$), 如何输入特征的维度很高,而且是稀疏线性关系的话，岭回归就不太合适，考虑使用 Lasso 回归。\n",
    "\n",
    "### 5.3 什么是 L1 正则化(Lasso回归)\n",
    "\n",
    "$L1$ 正则化与 $L2$ 正则化的区别在于惩罚项的不同：\n",
    "\n",
    "$J=J_0+\\lambda(|w_1|+|w_2|)$\n",
    "\n",
    "求解 $J_0$ 的过程可以画出等值线。同时L1正则化的函数也可以在 $w1$ $w2$ 的二维平面上画出来。如下图：\n",
    "\n",
    "![image](https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/01%20Linear%20Regression/image/2.jpg?raw=true)\n",
    "\n",
    "惩罚项表示为图中的黑色棱形，随着梯度下降法的不断逼近，与棱形第一次产生交点，而这个交点很容易出现在坐标轴上。**这就说明了L1正则化容易得到稀疏矩阵。**\n",
    "\n",
    "### 5.4 什么场景下使用 L1 正则化\n",
    "\n",
    "**$L1$ 正则化 (Lasso回归) 可以使得一些特征的系数变小,甚至还使一些绝对值较小的系数直接变为 $0$**，从而增强模型的泛化能力 。对于高的特征数据,尤其是线性关系是稀疏的，就采用 $L1$ 正则化 (Lasso 回归),或者是要在一堆特征里面找出主要的特征，那么 $L1$ 正则化 (Lasso 回归) 更是首选了。\n",
    "\n",
    "### 5.5 什么是 ElasticNet 回归\n",
    "\n",
    "**ElasticNet 综合了 $L1$ 正则化项和 $L2$ 正则化项**，以下是它的公式：\n",
    "\n",
    "$min(\\frac{1}{2m}[\\sum_{i=1}^{m}({y_i}^{'}-y_i)^2+\\lambda\\sum_{j=1}^{n}\\theta_j^2]+\\lambda\\sum_{j=1}^{n}|\\theta|)$\n",
    "\n",
    "### 5.6  ElasticNet 回归的使用场景\n",
    "\n",
    "ElasticNet 在我们发现用 Lasso 回归太过(太多特征被稀疏为$0$),而岭回归也正则化的不够(回归系数衰减太慢)的时候，可以考虑使用 ElasticNet 回归来综合，得到比较好的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 线性回归要求因变量服从正态分布？\n",
    "\n",
    "我们假设线性回归的噪声服从均值为$0$的正态分布。 当噪声符合正态分布 $N(0,\\delta^2)$ 时，因变量则符合正态分布$N(ax(i)+b,\\delta^2)$，其中预测函数 $y=ax(i)+b$。这个结论可以由正态分布的概率密度函数得到。也就是说当噪声符合正态分布时，其因变量必然也符合正态分布。 \n",
    "\n",
    "在用线性回归模型拟合数据之前，首先要求数据应符合或近似符合正态分布，否则得到的拟合函数不正确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. [代码实现](https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/01%20Linear%20Regression/demo/Linear%20Regression%20Coding.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
