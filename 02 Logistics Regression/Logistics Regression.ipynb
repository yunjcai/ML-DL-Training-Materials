{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 什么是逻辑回归\n",
    "\n",
    "逻辑回归是用来做分类算法的，大家都熟悉线性回归，一般形式是 $Y=aX+b$，y的取值范围是 [-∞, +∞]，有这么多取值，怎么进行分类呢？不用担心，伟大的数学家已经为我们找到了一个方法。\n",
    "\n",
    "也就是把Y的结果带入一个非线性变换的 **Sigmoid 函数**中，即可得到 [0,1] 之间取值范围的数 $S$，$S$ 可以把它看成是一个概率值，如果我们设置概率阈值为 0.5，那么 $S$ 大于 0.5 可以看成是正样本，小于 0.5 看成是负样本，就可以进行分类了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 什么是Sigmoid函数\n",
    "\n",
    "函数公式如下：\n",
    "\n",
    "![image](https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/02%20Logistics%20Regression/image/1.jpg?raw=true)\n",
    "\n",
    "函数中 $t$ 无论取什么值，其结果都在 [0,-1] 的区间内，回想一下，一个分类问题就有两种答案，一种是“是”，一种是“否”，那 0 对应着“否”，1 对应着“是”，那又有人问了，你这不是 [0,1] 的区间吗，怎么会只有 0 和 1 呢？这个问题问得好，我们假设分类的**阈值**是 0.5，那么超过 0.5 的归为 1 分类，低于 0.5 的归为 0 分类，阈值是可以自己设定的。\n",
    "\n",
    "好了，接下来我们把 $aX+b$ 带入 $t$ 中就得到了我们的逻辑回归的一般模型方程：\n",
    "\n",
    "$P(a,b)=\\frac{1}{1+e^{(aX+b)}}$\n",
    "\n",
    "结果 $P(a,b)$ 也可以理解为概率，换句话说概率大于 0.5 的属于 1 分类，概率小于 0.5 的属于 0 分类，这就达到了分类的目的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 损失函数是什么\n",
    "\n",
    "逻辑回归的损失函数是 **log loss**，也就是**对数似然函数**，函数公式如下：\n",
    "\n",
    "![image](https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/02%20Logistics%20Regression/image/2.jpg?raw=true)\n",
    "\n",
    "公式中的 $y=1$ 表示的是真实值为1时用第一个公式，真实 $y=0$ 用第二个公式计算损失。为什么要加上 $log$ 函数呢？可以试想一下，当真实样本为 1 是，但 $h=0$ 概率，那么 $log0=∞$，这就对模型最大的惩罚力度；当 $h=1$ 时，那么 $log1=0$，相当于没有惩罚，也就是没有损失，达到最优结果。所以数学家就想出了用 $log$ 函数来表示损失函数。\n",
    "\n",
    "最后按照梯度下降法一样，求解极小值点，得到想要的模型效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.可以进行多分类吗？\n",
    "\n",
    "可以的，其实我们可以从二分类问题过度到多分类问题 (one vs rest)，思路步骤如下：\n",
    "\n",
    "1.将类型 class1 看作正样本，其他类型全部看作负样本，然后我们就可以得到样本标记类型为该类型的概率 $p_1$。\n",
    "\n",
    "2.然后再将另外类型 class2 看作正样本，其他类型全部看作负样本，同理得到p2。\n",
    "\n",
    "3.以此循环，我们可以得到该待预测样本的标记类型分别为类型 class i 时的概率 $p_i$，最后我们取 $p_i$ 中最大的那个概率对应的样本标记类型作为我们的待预测样本类型。\n",
    "\n",
    "![image](https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/02%20Logistics%20Regression/image/3.jpg?raw=true)\n",
    "\n",
    "总之还是以二分类来依次划分，并求出最大概率结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.逻辑回归有什么优点\n",
    "\n",
    "- LR 能以概率的形式输出结果，而非只是 0,1 判定。\n",
    "- LR 的可解释性强，可控度高(你要给老板讲的嘛…)。\n",
    "- 训练快，feature engineering  之后效果赞。\n",
    "- 因为结果是概率，可以做 ranking model。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 逻辑回归有哪些应用\n",
    "\n",
    "- CTR 预估/推荐系统的 learning to rank 各种分类场景。\n",
    "- 某搜索引擎厂的广告 CTR 预估基线版是 LR。\n",
    "- 某电商搜索排序/广告 CTR 预估基线版是 LR。\n",
    "- 某电商的购物搭配推荐用了大量 LR。\n",
    "- 某现在一天广告赚 1000w+ 的新闻 app 排序基线是 LR。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 逻辑回归常用的优化方法有哪些\n",
    "\n",
    "### 7.1 一阶方法\n",
    "\n",
    "梯度下降、随机梯度下降、mini 随机梯度下降降法。随机梯度下降不但速度上比原始梯度下降要快，局部最优化问题时可以一定程度上抑制局部最优解的发生。 \n",
    "\n",
    "### 7.2 二阶方法：牛顿法、拟牛顿法： \n",
    "\n",
    "这里详细说一下牛顿法的基本原理和牛顿法的应用方式。牛顿法其实就是通过切线与 x 轴的交点不断更新切线的位置，直到达到曲线与 x 轴的交点得到方程解。在实际应用中我们因为常常要求解凸优化问题，也就是要求解函数一阶导数为 0 的位置，而牛顿法恰好可以给这种问题提供解决方法。实际应用中牛顿法首先选择一个点作为起始点，并进行一次二阶泰勒展开得到导数为 0 的点进行一个更新，直到达到要求，这时牛顿法也就成了二阶求解问题，比一阶方法更快。我们常常看到的 x 通常为一个多维向量，这也就引出了 Hessian 矩阵的概念（就是 x 的二阶导数矩阵）。\n",
    "\n",
    "缺点：牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定的下降，严重时甚至会失败。还有就是牛顿法要求函数一定是二阶可导的。而且计算 Hessian 矩阵的逆复杂度很大。\n",
    "\n",
    "拟牛顿法： 不用二阶偏导而是构造出 Hessian 矩阵的近似正定对称矩阵的方法称为拟牛顿法。拟牛顿法的思路就是用一个特别的表达形式来模拟 Hessian 矩阵或者是他的逆使得表达式满足拟牛顿条件。主要有 DFP 法（逼近 Hession 的逆）、BFGS（直接逼近 Hession 矩阵）、 L-BFGS（可以减少 BFGS 所需的存储空间）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 逻辑斯特回归为什么要对特征进行离散化。\n",
    "\n",
    "1. 非线性！非线性！非线性！逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散特征的增加和减少都很容易，易于模型的快速迭代； \n",
    "2. 速度快！速度快！速度快！稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展； \n",
    "3. 鲁棒性！鲁棒性！鲁棒性！离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄 >30 是 1，否则 0。如果特征没有离散化，一个异常数据“年龄 300 岁”会给模型造成很大的干扰； \n",
    "4. 方便交叉与特征组合：离散化后可以进行特征交叉，由 M+N 个变量变为 MxN 个变量，进一步引入非线性，提升表达能力； \n",
    "5. 稳定性：特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30 作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问； \n",
    "6. 简化模型：特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 逻辑回归的目标函数中增大L1正则化会是什么结果。\n",
    "\n",
    "所有的参数 $w$ 都会变成 0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. [代码实现](https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/01%20Linear%20Regression/demo/Linear%20Regression%20Coding.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
