{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gradient Descent - Problem of Hiking Down a Mountain](https://storage.googleapis.com/supplemental_media/udacityu/315142919/Gradient%20Descent.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度下降\n",
    "\n",
    "梯度下降的基本过程就和下山的场景很类似。\n",
    "\n",
    "首先，我们有一个可微分的函数。这个函数就代表着一座山。我们的目标就是找到这个函数的最小值，也就是山底。最快的下山的方式就是找到当前位置最陡峭的方向，然后沿着此方向向下走，对应到函数中，就是找到给定点的梯度（斜率），然后朝着梯度相反的方向，就能让函数值下降的最快！因为梯度的方向就是函数之变化最快的方向(在后面会详细解释)。\n",
    "\n",
    "所以，我们重复利用这个方法，反复求取梯度，最后就能到达局部的最小值，这就类似于我们下山的过程。而求取梯度就确定了最陡峭的方向，也就是场景中测量方向的手段。那么为什么梯度的方向就是最陡峭的方向呢？\n",
    "\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/Gradient%20Descent/image/1.jpg?raw=true\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "接下来，我们从微分开始讲起\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 微分\n",
    "\n",
    "看待微分的意义，可以有不同的角度，最常用的两种是：\n",
    "\n",
    "* 函数图像中，某点的切线的斜率\n",
    "* 函数的变化率\n",
    "\n",
    "几个微分的例子\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/Gradient%20Descent/image/2.jpg?raw=true\" alt=\"drawing\" width=\"250\"/>\n",
    "\n",
    "上面的例子都是单变量的微分，当一个函数有多个变量的时候，就有了多变量的微分，即分别对每个变量进行求微分\n",
    "\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/Gradient%20Descent/image/3.jpg?raw=true\" alt=\"drawing\" width=\"350\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度\n",
    "\n",
    "梯度实际上就是多变量微分的一般化。\n",
    "\n",
    "下面这个例子：\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/Gradient%20Descent/image/4.jpg?raw=true\" alt=\"drawing\" width=\"350\"/>\n",
    "\n",
    "我们可以看到，梯度就是分别对每个变量进行微分，然后用逗号分割开，梯度是用<>包括起来，说明梯度其实一个向量。\n",
    "\n",
    "* 在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率\n",
    "* 在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向\n",
    "\n",
    "我们需要到达山底，就需要在每一步观测到此时最陡峭的地方，梯度就恰巧告诉了我们这个方向。梯度的方向是函数在给定点上升最快的方向，那么梯度的反方向就是函数在给定点下降最快的方向，这正是我们所需要的。所以我们只要沿着梯度的方向一直走，就能走到局部的最低点！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度下降算法的数学解释\n",
    "\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/Gradient%20Descent/image/5.jpg?raw=true\" alt=\"drawing\" width=\"350\"/>\n",
    "\n",
    "此公式的意义是：$J$ 是关于 $Θ$ 的一个函数，我们当前所处的位置为 $Θ0$ 点，要从这个点走到 $J$ 的最小值点，也就是山底。首先我们先确定前进的方向，也就是梯度的反向，然后走一段距离的步长，也就是 $α$，走完这个段步长，就到达了 $Θ1$ 这个点！\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/Gradient%20Descent/image/6.jpg?raw=true\" alt=\"drawing\" width=\"450\"/>\n",
    "\n",
    "##### α是什么含义？\n",
    "\n",
    "α在梯度下降算法中被称作为学习率或者步长，意味着我们可以通过α来控制每一步走的距离，以保证不要步子跨的太大扯着蛋，哈哈，其实就是不要走太快，错过了最低点。同时也要保证不要走的太慢，导致太阳下山了，还没有走到山下。所以α的选择在梯度下降法中往往是很重要的！α不能太大也不能太小，太小的话，可能导致迟迟走不到最低点，太大的话，会导致错过最低点！\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/Gradient%20Descent/image/7.jpg?raw=true\" alt=\"drawing\" width=\"450\"/>\n",
    "\n",
    "##### 为什么要梯度要乘以一个负号？\n",
    "梯度前加一个负号，就意味着朝着梯度相反的方向前进！我们在前文提到，梯度的方向实际就是函数在此点上升最快的方向！而我们需要朝着下降最快的方向走，自然就是负的梯度的方向，所以此处需要加上负号"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度下降算法的实例\n",
    "\n",
    "我们已经基本了解了梯度下降算法的计算过程，那么我们就来看几个梯度下降算法的小实例，首先从单变量的函数开始\n",
    "\n",
    "## 单变量函数的梯度下降\n",
    "\n",
    "我们假设有一个单变量的函数\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/Gradient%20Descent/image/8.jpg?raw=true\" alt=\"drawing\" width=\"100\"/>\n",
    "\n",
    "函数的微分\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/Gradient%20Descent/image/9.jpg?raw=true\" alt=\"drawing\" width=\"100\"/>\n",
    "\n",
    "初始化，起点为\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/Gradient%20Descent/image/10.jpg?raw=true\" alt=\"drawing\" width=\"75\"/>\n",
    "\n",
    "学习率为\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/Gradient%20Descent/image/11.jpg?raw=true\" alt=\"drawing\" width=\"75\"/>\n",
    "\n",
    "根据梯度下降的计算公式\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/Gradient%20Descent/image/6.jpg?raw=true\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "我们开始进行梯度下降的迭代计算过程：\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/Gradient%20Descent/image/12.jpg?raw=true\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "如图，经过四次的运算，也就是走了四步，基本就抵达了函数的最低点，也就是山底\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/Gradient%20Descent/image/13.jpg?raw=true\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "## 多变量函数的梯度下降\n",
    "\n",
    "我们假设有一个目标函数\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/Gradient%20Descent/image/14.jpg?raw=true\" alt=\"drawing\" width=\"150\"/>\n",
    "\n",
    "现在要通过梯度下降法计算这个函数的最小值。我们通过观察就能发现最小值其实就是 $(0，0)$ 点。但是接下来，我们会从梯度下降算法开始一步步计算到这个最小值！\n",
    "\n",
    "我们假设初始的起点为：\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/Gradient%20Descent/image/15.jpg?raw=true\" alt=\"drawing\" width=\"100\"/>\n",
    "\n",
    "初始的学习率为：\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/Gradient%20Descent/image/16.jpg?raw=true\" alt=\"drawing\" width=\"75\"/>\n",
    "\n",
    "函数的梯度为：\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/Gradient%20Descent/image/17.jpg?raw=true\" alt=\"drawing\" width=\"150\"/>\n",
    "\n",
    "进行多次迭代：\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/Gradient%20Descent/image/18.jpg?raw=true\" alt=\"drawing\" width=\"450\"/>\n",
    "\n",
    "我们发现，已经基本靠近函数的最小值点\n",
    "<img src=\"https://github.com/yunjcai/ML-DL-Training-Materials/blob/main/Gradient%20Descent/image/19.jpg?raw=true\" alt=\"drawing\" width=\"400\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
